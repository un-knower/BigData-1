##测试环境 配置
##192.168.2.43:2181,192.168.2.45:2181,192.168.2.46:2181

#filechannel备份
agent1.sources  = kafkaPacket  kafkaForward thriftAlarmRealInfo
agent1.channels = hdfsChannel alarmChannel realInfoChannel packetChannel errorPacketChannel forwardChannel
agent1.sinks    = hdfsSink0 hdfsSink1 hdfsSink2 realInfoSink0 realInfoSink1 realInfoSink2 alarmSink  packetSink0 packetSink1 packetSink2  forwardSink0 forwardSink1 forwardSink2

#hdfs text  channel 实时信息和报警
agent1.channels.hdfsChannel.type = memory
agent1.channels.hdfsChannel.checkpointDir=/home/flume/channel/hdfsChannel/checkpoint
agent1.channels.hdfsChannel.dataDirs=/home/flume/channel/hdfsChannel/data
agent1.channels.hdfsChannel.capacity=200000000
agent1.channels.hdfsChannel.transactionCapacity=60000
agent1.channels.hdfsChannel.checkpointInterval=60000
#报警channel 写入hbase
agent1.channels.alarmChannel.type = memory
agent1.channels.alarmChannel.checkpointDir=/home/flume/channel/alarmChannel/checkpoint
agent1.channels.alarmChannel.dataDirs=/home/flume/channel/alarmChannel/data
agent1.channels.alarmChannel.capacity=200000000
agent1.channels.alarmChannel.transactionCapacity=60000
agent1.channels.alarmChannel.checkpointInterval=60000
#实时信息channel 写入hbase
agent1.channels.realInfoChannel.type = memory
agent1.channels.realInfoChannel.checkpointDir=/home/flume/channel/realInfoChannel/checkpoint
agent1.channels.realInfoChannel.dataDirs=/home/flume/channel/realInfoChannel/data
agent1.channels.realInfoChannel.capacity=200000000
agent1.channels.realInfoChannel.transactionCapacity=60000
agent1.channels.realInfoChannel.checkpointInterval=60000
#原始报文channel 写入hbase
agent1.channels.packetChannel.type = memory
agent1.channels.packetChannel.checkpointDir=/home/flume/channel/packetChannel/checkpoint
agent1.channels.packetChannel.dataDirs=/home/flume/channel/packetChannel/data
agent1.channels.packetChannel.capacity=200000000
agent1.channels.packetChannel.transactionCapacity=60000
agent1.channels.packetChannel.checkpointInterval=60000
#错误的原始报文channel 写入hbase
agent1.channels.errorPacketChannel.type = memory
agent1.channels.errorPacketChannel.checkpointDir=/home/flume/channel/errorPacketChannel/checkpoint
agent1.channels.errorPacketChannel.dataDirs=/home/flume/channel/errorPacketChannel/data
agent1.channels.errorPacketChannel.capacity=200000000
agent1.channels.errorPacketChannel.transactionCapacity=60000
agent1.channels.errorPacketChannel.checkpointInterval=60000
#转发记录channel 写入hbase
agent1.channels.forwardChannel.type = memory
agent1.channels.forwardChannel.checkpointDir=/home/flume/channel/forwardChannel/checkpoint
agent1.channels.forwardChannel.dataDirs=/home/flume/channel/forwardChannel/data
agent1.channels.forwardChannel.capacity=200000000
agent1.channels.forwardChannel.transactionCapacity=60000
agent1.channels.forwardChannel.checkpointInterval=60000

#source
#原始报文source
agent1.sources.kafkaPacket.type=com.zjhl.flume.source.kafka.KafkaPacketSource2
agent1.sources.kafkaPacket.zookeeperConnect=192.168.2.43:2181,192.168.2.45:2181,192.168.2.46:2181
agent1.sources.kafkaPacket.groupId=g_rawdata
agent1.sources.kafkaPacket.topic=us_packet
agent1.sources.kafkaPacket.batchDurationMillis=1000
agent1.sources.kafkaPacket.batchSize=1000
#选择器配置
agent1.sources.kafkaPacket.channels=packetChannel errorPacketChannel
agent1.sources.kafkaPacket.selector.type=multiplexing
agent1.sources.kafkaPacket.selector.header=CHANNEL_KEY
agent1.sources.kafkaPacket.selector.mapping.PACKET=packetChannel
agent1.sources.kafkaPacket.selector.mapping.PACKET_ERROR=errorPacketChannel


#转发记录source
agent1.sources.kafkaForward.type=com.zjhl.flume.source.kafka.KafkaForwardSource2
agent1.sources.kafkaForward.zookeeperConnect=192.168.2.43:2181,192.168.2.45:2181,192.168.2.46:2181
agent1.sources.kafkaForward.groupId=g_forward
agent1.sources.kafkaForward.topic=us_forward
agent1.sources.kafkaForward.batchDurationMillis=1000
agent1.sources.kafkaForward.batchSize=1000
#转发记录source选择器
agent1.sources.kafkaForward.channels=forwardChannel hdfsChannel
agent1.sources.kafkaForward.selector.type=multiplexing
agent1.sources.kafkaForward.selector.header=CHANNEL_KEY
agent1.sources.kafkaForward.selector.mapping.FORWARD=forwardChannel
agent1.sources.kafkaForward.selector.mapping.HDFS=hdfsChannel

#thrift  source  负责接收实时信息和报警信息
agent1.sources.thriftAlarmRealInfo.type=com.zjhl.flume.source.thrift.ZJHLThriftSource
agent1.sources.thriftAlarmRealInfo.bind=0.0.0.0
agent1.sources.thriftAlarmRealInfo.port=1463
agent1.sources.thriftAlarmRealInfo.threads=3
agent1.sources.thriftAlarmRealInfo.channels=hdfsChannel alarmChannel realInfoChannel forwardChannel
agent1.sources.thriftAlarmRealInfo.selector.type=multiplexing

#选择器的key值
agent1.sources.thriftAlarmRealInfo.selector.header=CHANNEL_KEY
agent1.sources.thriftAlarmRealInfo.selector.mapping.ALARM=alarmChannel
agent1.sources.thriftAlarmRealInfo.selector.mapping.REALINFO=realInfoChannel
agent1.sources.thriftAlarmRealInfo.selector.mapping.HDFS=hdfsChannel
agent1.sources.thriftAlarmRealInfo.selector.mapping.FORWARD=forwardChannel

#sink1
#原始报文hbase packetSink
agent1.sinks.packetSink0.channel=packetChannel
agent1.sinks.packetSink0.type=hbase
agent1.sinks.packetSink0.table=packet
agent1.sinks.packetSink0.columnFamily =cf
agent1.sinks.packetSink0.batchSize = 1000
agent1.sinks.packetSink0.timeout=1000
agent1.sinks.packetSink0.znodeParent=/hbase
agent1.sinks.packetSink0.zookeeperQuorum=192.168.2.43:2181,192.168.2.45:2181,192.168.2.46:2181
agent1.sinks.packetSink0.serializer =com.zjhl.flume.hbase.serializer.Packet

agent1.sinks.packetSink1.channel=packetChannel
agent1.sinks.packetSink1.type=hbase
agent1.sinks.packetSink1.table=packet
agent1.sinks.packetSink1.columnFamily =cf
agent1.sinks.packetSink1.batchSize = 1000
agent1.sinks.packetSink1.timeout=1000
agent1.sinks.packetSink1.znodeParent=/hbase
agent1.sinks.packetSink1.zookeeperQuorum=192.168.2.43:2181,192.168.2.45:2181,192.168.2.46:2181
agent1.sinks.packetSink1.serializer =com.zjhl.flume.hbase.serializer.Packet

#packsink2
agent1.sinks.packetSink2.channel=packetChannel
agent1.sinks.packetSink2.type=hbase
agent1.sinks.packetSink2.table=packet
agent1.sinks.packetSink2.columnFamily =cf
agent1.sinks.packetSink2.batchSize = 1000
agent1.sinks.packetSink2.timeout=1000
agent1.sinks.packetSink2.znodeParent=/hbase
agent1.sinks.packetSink2.zookeeperQuorum=192.168.2.43:2181,192.168.2.45:2181,192.168.2.46:2181
agent1.sinks.packetSink2.serializer =com.zjhl.flume.hbase.serializer.Packet

#错误的原始报文hbase packetSink
agent1.sinks.errorPacketSink.channel=errorPacketChannel
agent1.sinks.errorPacketSink.type=hbase
agent1.sinks.errorPacketSink.table=packet
agent1.sinks.errorPacketSink.columnFamily =cf
agent1.sinks.errorPacketSink.batchSize = 1000
agent1.sinks.errorPacketSink.timeout=1000
agent1.sinks.errorPacketSink.znodeParent=/hbase
agent1.sinks.errorPacketSink.zookeeperQuorum=192.168.2.43:2181,192.168.2.45:2181,192.168.2.46:2181
agent1.sinks.errorPacketSink.serializer =com.zjhl.flume.hbase.serializer.ErrorPacket
#实时信息 hbase realInfoSink0
agent1.sinks.realInfoSink0.channel=realInfoChannel
agent1.sinks.realInfoSink0.type=hbase
agent1.sinks.realInfoSink0.table=realinfo
agent1.sinks.realInfoSink0.columnFamily =cf
agent1.sinks.realInfoSink0.batchSize = 1000
agent1.sinks.realInfoSink0.timeout=1000
agent1.sinks.realInfoSink0.znodeParent=/hbase
agent1.sinks.realInfoSink0.zookeeperQuorum=192.168.2.43:2181,192.168.2.45:2181,192.168.2.46:2181
agent1.sinks.realInfoSink0.serializer =com.zjhl.flume.hbase.serializer.RealInfo
agent1.sinks.realInfoSink0.compress =true

agent1.sinks.realInfoSink1.channel=realInfoChannel
agent1.sinks.realInfoSink1.type=hbase
agent1.sinks.realInfoSink1.table=realinfo
agent1.sinks.realInfoSink1.columnFamily =cf
agent1.sinks.realInfoSink1.batchSize = 1000
agent1.sinks.realInfoSink1.timeout=1000
agent1.sinks.realInfoSink1.znodeParent=/hbase
agent1.sinks.realInfoSink1.zookeeperQuorum=192.168.2.43:2181,192.168.2.45:2181,192.168.2.46:2181
agent1.sinks.realInfoSink1.serializer =com.zjhl.flume.hbase.serializer.RealInfo
agent1.sinks.realInfoSink1.compress = true

#realInfoSink2
agent1.sinks.realInfoSink2.channel=realInfoChannel
agent1.sinks.realInfoSink2.type=hbase
agent1.sinks.realInfoSink2.table=realinfo
agent1.sinks.realInfoSink2.columnFamily =cf
agent1.sinks.realInfoSink2.batchSize = 1000
agent1.sinks.realInfoSink2.timeout=1000
agent1.sinks.realInfoSink2.znodeParent=/hbase
agent1.sinks.realInfoSink2.zookeeperQuorum=192.168.2.43:2181,192.168.2.45:2181,192.168.2.46:2181
agent1.sinks.realInfoSink2.serializer =com.zjhl.flume.hbase.serializer.RealInfo
agent1.sinks.realInfoSink2.compress = true

#报警信息 hbase alarmSink
agent1.sinks.alarmSink.channel=alarmChannel
agent1.sinks.alarmSink.type=hbase
agent1.sinks.alarmSink.table=alarm
agent1.sinks.alarmSink.columnFamily =cf
agent1.sinks.alarmSink.batchSize = 1000
agent1.sinks.alarmSink.timeout=1000
agent1.sinks.alarmSink.znodeParent=/hbase
agent1.sinks.alarmSink.zookeeperQuorum=192.168.2.43:2181,192.168.2.45:2181,192.168.2.46:2181
agent1.sinks.alarmSink.serializer =com.zjhl.flume.hbase.serializer.Alarm

#转发记录 hbase forwardSink0
agent1.sinks.forwardSink0.channel=forwardChannel
agent1.sinks.forwardSink0.type=hbase
agent1.sinks.forwardSink0.table=forward
agent1.sinks.forwardSink0.columnFamily =cf
agent1.sinks.forwardSink0.batchSize = 1000
agent1.sinks.forwardSink0.timeout=1000
agent1.sinks.forwardSink0.znodeParent=/hbase
agent1.sinks.forwardSink0.zookeeperQuorum=192.168.2.43:2181,192.168.2.45:2181,192.168.2.46:2181
agent1.sinks.forwardSink0.serializer =com.zjhl.flume.hbase.serializer.Forward

#转发记录 hbase forwardSink1
agent1.sinks.forwardSink1.channel=forwardChannel
agent1.sinks.forwardSink1.type=hbase
agent1.sinks.forwardSink1.table=forward
agent1.sinks.forwardSink1.columnFamily =cf
agent1.sinks.forwardSink1.batchSize = 1000
agent1.sinks.forwardSink1.timeout=1000
agent1.sinks.forwardSink1.znodeParent=/hbase
agent1.sinks.forwardSink1.zookeeperQuorum=192.168.2.43:2181,192.168.2.45:2181,192.168.2.46:2181
agent1.sinks.forwardSink1.serializer =com.zjhl.flume.hbase.serializer.Forward

#转发记录 hbase forwardSink2
agent1.sinks.forwardSink2.channel=forwardChannel
agent1.sinks.forwardSink2.type=hbase
agent1.sinks.forwardSink2.table=forward
agent1.sinks.forwardSink2.columnFamily =cf
agent1.sinks.forwardSink2.batchSize = 1000
agent1.sinks.forwardSink2.timeout=1000
agent1.sinks.forwardSink2.znodeParent=/hbase
agent1.sinks.forwardSink2.zookeeperQuorum=192.168.2.43:2181,192.168.2.45:2181,192.168.2.46:2181
agent1.sinks.forwardSink2.serializer =com.zjhl.flume.hbase.serializer.Forward

#hdfs hdfsSink0
agent1.sinks.hdfsSink0.channel=hdfsChannel
agent1.sinks.hdfsSink0.type = hdfs
#agent1.sinks.hdfsSink0.hdfs.path = hdfs://nameservice1/vehicle/data/%{category}
agent1.sinks.hdfsSink0.hdfs.path = hdfs://dfs.namenode.servicerpc-address/vehicle/data/%{category}
#agent1.sinks.hdfsSink0.hdfs.filePrefix=host81_0
agent1.sinks.hdfsSink0.hdfs.filePrefix=test01
#agent1.sinks.hdfsSink0.hdfs.fileType = CompressedStream
agent1.sinks.hdfsSink0.hdfs.fileType = DataStream
agent1.sinks.hdfsSink0.hdfs.codeC = lzop
agent1.sinks.hdfsSink0.hdfs.writeFormat = Text
agent1.sinks.hdfsSink0.hdfs.rollInterval=0
agent1.sinks.hdfsSink0.hdfs.rollSize = 1000000000
agent1.sinks.hdfsSink0.hdfs.rollCount = 0
agent1.sinks.hdfsSink0.hdfs.batchSize=1000
agent1.sinks.hdfsSink0.hdfs.threadsPoolSize=10
agent1.sinks.hdfsSink0.hdfs.callTimeout=30000
agent1.sinks.hdfsSink0.hdfs.idleTimeout=300
agent1.sinks.hdfsSink0.hdfs.proxyUser=hadoop
agent1.sinks.hdfsSink0.hdfs.retryInterval = 3

#hdfs hdfsSink1
agent1.sinks.hdfsSink1.channel=hdfsChannel
agent1.sinks.hdfsSink1.type = hdfs
agent1.sinks.hdfsSink1.hdfs.path = hdfs://nameservice1/vehicle/data/%{category}
agent1.sinks.hdfsSink0.hdfs.path = hdfs://dfs.namenode.servicerpc-address/vehicle/data/%{category}
#agent1.sinks.hdfsSink1.hdfs.filePrefix=host81_1
agent1.sinks.hdfsSink1.hdfs.filePrefix=test03
agent1.sinks.hdfsSink1.hdfs.fileType = DataStream
agent1.sinks.hdfsSink1.hdfs.codeC = lzop
agent1.sinks.hdfsSink1.hdfs.writeFormat = Text
agent1.sinks.hdfsSink1.hdfs.rollInterval=0
agent1.sinks.hdfsSink1.hdfs.rollSize = 1000000000
agent1.sinks.hdfsSink1.hdfs.rollCount = 0
agent1.sinks.hdfsSink1.hdfs.batchSize=1000
agent1.sinks.hdfsSink1.hdfs.threadsPoolSize=10
agent1.sinks.hdfsSink1.hdfs.callTimeout=30000
agent1.sinks.hdfsSink1.hdfs.idleTimeout=300
agent1.sinks.hdfsSink1.hdfs.proxyUser=hadoop
agent1.sinks.hdfsSink1.hdfs.retryInterval = 3

#hdfs hdfsSink2
agent1.sinks.hdfsSink2.channel=hdfsChannel
agent1.sinks.hdfsSink2.type = hdfs
agent1.sinks.hdfsSink2.hdfs.path = hdfs://nameservice1/vehicle/data/%{category}
agent1.sinks.hdfsSink0.hdfs.path = hdfs://dfs.namenode.servicerpc-address/vehicle/data/%{category}
#agent1.sinks.hdfsSink2.hdfs.filePrefix=host81_2
agent1.sinks.hdfsSink1.hdfs.filePrefix=test04
agent1.sinks.hdfsSink2.hdfs.fileType = DataStream
agent1.sinks.hdfsSink2.hdfs.codeC = lzop
agent1.sinks.hdfsSink2.hdfs.writeFormat = Text
agent1.sinks.hdfsSink2.hdfs.rollInterval=0
agent1.sinks.hdfsSink2.hdfs.rollSize = 1000000000
agent1.sinks.hdfsSink2.hdfs.rollCount = 0
agent1.sinks.hdfsSink2.hdfs.batchSize=1000
agent1.sinks.hdfsSink2.hdfs.threadsPoolSize=10
agent1.sinks.hdfsSink2.hdfs.callTimeout=30000
agent1.sinks.hdfsSink2.hdfs.idleTimeout=300
agent1.sinks.hdfsSink2.hdfs.proxyUser=hadoop
agent1.sinks.hdfsSink2.hdfs.retryInterval = 3
